{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"19H2N94OGlUY"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchvision import datasets, transforms\n","import torch.functional as F\n","from tqdm import tqdm\n"]},{"cell_type":"markdown","metadata":{"id":"P3pfFsWOZnP7"},"source":["# Linear layer network (fully-connected, dense layer)\n","\n","torch.nn.module을 사용하여 linear model initialization을 할 때, 아래와 같은 argument를 정해줘야 합니다.\n","\n","- in_features (necessary): input vector dimension을 나타내는 `integer` number\n","- out_features (necessary): output vector dimension을 나타내는 `integer` number\n","- bias: bias parameter를 사용할지 말지에 대한 `bool` type number\n","- device: training, inference에 어떤 computational resource를 사용할지에 대한 argument. 보통 `cpu` or `cuda` 를 사용\n","- dtype: 모델 파라미터에 사용할 number의 data type으로, 보통 `float32`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UcDmktbZnP9"},"outputs":[],"source":["input_size = 784\n","output_size = 10\n","\n","linear = nn.Linear(\n","    in_features=input_size, \n","    out_features=output_size,\n","    bias=True,\n","    device='cpu',\n","    dtype=torch.float32\n","    )"]},{"cell_type":"markdown","metadata":{"id":"DmOvLbweZnP9"},"source":["linear network가 어떤 hyperparameter로 initialization 되었는지 확인합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0bNRnzIZnP-","outputId":"0b32e1ad-84e3-4a2c-8d6f-4a42dc91aa1b"},"outputs":[{"data":{"text/plain":["Linear(in_features=784, out_features=10, bias=True)"]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["linear"]},{"cell_type":"markdown","metadata":{"id":"tEtIj4l1ZnP-"},"source":["weight, bias의 tensor size를 확인합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUTD6pl9ZnP_","outputId":"201c1189-55df-488b-8942-f35a104bce6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["size of weight parameters: torch.Size([10, 784])\n"," size of bias parameters: torch.Size([10])\n"]}],"source":["params = list(linear.parameters())\n","print(\n","    f\"size of weight parameters: {params[0].size()}\\n\",\n","    f\"size of bias parameters: {params[1].size()}\",\n","    )"]},{"cell_type":"markdown","metadata":{"id":"T-jqfhlnZnP_"},"source":["tensor of random numbers를 사용해서, initialization된 linear network를 어떻게 쓸 수 있는지 확인해보겠습니다.\n","\n","먼저, 아래와 같은 debugging용 데이터를 생성합니다.\n","\n","- X: random tensor of (batch_size, input_size)\n","\n","- Y: arbitrary classification label fixed at 0.\n","\n","Cross-entropy loss 와 stochastic gradient descent optimizer 를 사용하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZ__BAJmZnP_"},"outputs":[],"source":["batch_size = 12\n","\n","# define example input and label\n","input = torch.randn(batch_size, input_size, requires_grad=True)\n","label = torch.zeros(batch_size, dtype=torch.long)\n","\n","# define loss function for classification problem\n","loss_function = nn.CrossEntropyLoss()\n","\n","# define optimizer to update parameters given gradients\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"]},{"cell_type":"markdown","metadata":{"id":"SNu9EE8_ZnP_"},"source":["tensor 연산을 할 때, 모든 tensor는 동일한 device에 올라가 있어야 합니다.\n","\n","model parameters와 the input tensor 또한 tensor 연산에 사용될 것이기 때문에 같은 device로 옮겨줄 필요가 있습니다.\n","\n","예를 들어, 만약 model parameters는 `cpu`에, input tensor는 `cuda`에 올라가 있다면 에러가 나게 됩니다.\n","\n","이 튜토리얼에서는 `cpu`로 device를 통일하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1VXdcwfZnP_","outputId":"40feb545-c3fe-4419-d96d-5e6380c27053"},"outputs":[{"name":"stdout","output_type":"stream","text":["input device: cpu\n"," model device: cpu\n"]}],"source":["# check whether the input and model parameters are on the same device\n","print(f\"input device: {input.device}\\n\", f\"model device: {linear.weight.device}\")"]},{"cell_type":"markdown","metadata":{"id":"V6QMQDxQZnQA"},"source":["## Forward propagation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xla1q40oZnQA"},"outputs":[],"source":["# call the forward function of the instance, linear\n","out = linear(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy-vhrFCZnQA","outputId":"625a77f3-edce-4428-e1dd-83252b251bb8"},"outputs":[{"data":{"text/plain":["tensor(2.4582, grad_fn=<NllLossBackward0>)"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["# calculate cross entropy loss between the probability distribution out and the label\n","loss = loss_function(out, label)\n","\n","# check the loss value\n","loss"]},{"cell_type":"markdown","metadata":{"id":"yoiQYQzKZnQA"},"source":["# Automatic backpropagation using autograd\n","\n","딥러닝 모델의 파라미터 업데이트를 위해서는 backpropagation 알고리즘이 구현되어 있어야 하는데요.\n","\n","torch의 `autograd` library를 사용함으로써, backpropagation을 구현하지 않고도 모델 업데이트를 편리하게 할 수 있습니다.\n","\n","`torch.nn` module은 단순히 `forward` 과정에서 모델 파라미터들이 loss function 계산에 참여하는 과정을 자동으로 추적한 뒤, backpropagation이 가능하도록 미분해줍니다.\n"]},{"cell_type":"markdown","metadata":{"id":"_ANaUsPdA-5g"},"source":["먼저, backpropagation을 하기 전에, 혹시나 남아있을 수 있는 gradient를 `zero_grad()`를 사용하여 초기화한 뒤, 남아 있는 gradient가 있는지 확인합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWkmhdbCZnQA"},"outputs":[],"source":["# before do backpropagation, we need to zero the gradient\n","linear.zero_grad()\n","\n","# check the gradient before backpropagation\n","linear.weight.grad"]},{"cell_type":"markdown","metadata":{"id":"i85R8QJzA-5h"},"source":["이제, backward() method를 호출하여 gradient 정보를 업데이트해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HG_sB7vDZnQA","outputId":"ce299e35-2422-442d-fcfb-6d5c203cd181"},"outputs":[{"data":{"text/plain":["tensor([[ 0.4533, -0.4148,  0.2939,  ..., -0.1157,  0.0359,  0.2141],\n","        [-0.0746,  0.0418, -0.0315,  ...,  0.0158, -0.0022, -0.0087],\n","        [-0.0465,  0.0325, -0.0288,  ...,  0.0125, -0.0085,  0.0018],\n","        ...,\n","        [-0.0494,  0.0545, -0.0378,  ...,  0.0194, -0.0065,  0.0075],\n","        [-0.0352,  0.0202, -0.0250,  ...,  0.0111, -0.0182, -0.0106],\n","        [-0.0218,  0.0653, -0.0575,  ...,  0.0380,  0.0257, -0.0833]])"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["# do backpropagation\n","loss.backward()\n","\n","# check the gradient after backpropagation\n","linear.weight.grad"]},{"cell_type":"markdown","metadata":{"id":"uOuixBApA-5h"},"source":["`loss`를 계산하는 과정에서 추적한 gradient 정보를 사용해, `backward()` 함수를 호출함으로써 backpropagation을 실행합니다.\n","\n","각 model parameter들은 `loss` 계산에서 기여한 만큼의 gradient를 저장하게 됩니다.\n","\n","backpropagation 은 gradient 계산까지만을 의미하며, 이 시점에서는 아직 model parameter들은 업데이트되지 않았습니다.\n","\n","추후의 비교를 위해, 업데이트 되기 전의 model parameter를 복사해둡니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0zd0zb2A-5h"},"outputs":[],"source":["previous_linear_weight = linear.weight.clone()"]},{"cell_type":"markdown","metadata":{"id":"1tFKvSMCZnQB"},"source":["linear model의 parameter를 `backward()`로 계산되었던 gradient 및 SGD optimizer를 사용하여 업데이트해보겠습니다.\n","\n","방법은 아주 간단한데, `optimizer`의 `step()` method를 호출하는 것입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyNGs7b6ZnQB"},"outputs":[],"source":["\n","optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"MsvSVzZXA-5h"},"source":["업데이트 이후,\n","\n","- `previous_model.parameter`\n","- `updated_model.parameter`\n","\n","가 각각 달라졌는지 확인해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XV1nAvowZnQB","outputId":"c9fb2e8d-8f2b-464b-c89c-4029722140c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["# check equality of the tensors after update\n","print(torch.equal(linear.weight, previous_linear_weight))"]},{"cell_type":"markdown","metadata":{"id":"rk8T3H-_ZnQB"},"source":["# Multi-Layer Perceptron (MLP)"]},{"cell_type":"markdown","metadata":{"id":"_gUxR0wVZnQC"},"source":["## Neural Network building using torch.nn module\n","\n","`torch.nn` module을 사용해 deep neural network를 정의해줄 수 있습니다.\n","\n","deep neural network는 parameter update에 gradient based optimization을 사용합니다.\n","\n","이를 위해서는 gradient backpropagation algorithm이 구현되어 있어야하는데요.\n","\n","`torch.nn` module을 사용할 경우, `autograd`를 통한 gradient 계산을 `torch` library가 자동으로 해주기 때문에 편리합니다.\n","\n","아래는 그림은 Multi-Layer Perceptron의 구조에 대한 그림입니다.\n","\n","![image](https://drive.google.com/uc?export=download&id=1YDra8zUcoZHUypmgPwfGKQtef7HFT8UP)\n","\n","`torch.nn`을 사용해서 Multi-Layer Perceptron model을 정의해보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY3dGZCmksSg","outputId":"a47b79dd-7e25-4896-d06d-d6c4794efaea"},"outputs":[{"data":{"text/plain":["MLP(\n","  (fc1): Linear(in_features=784, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n","  (softmax): Softmax(dim=-1)\n",")"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(784, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 10)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], -1)\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.softmax(self.fc3(x))\n","        return x\n","    \n","mlp = MLP()\n","mlp\n"]},{"cell_type":"markdown","metadata":{"id":"eREqQ3BeA-5i"},"source":["`class MLP(nn.Module):`\n","\n","nn.Module class를 상속받은 MLP model을 선언합니다.\n","\n","nn.Module을 상속받아야만 `autograd` 등 여러 유용한 기능을 사용할 수 있습니다.\n","\n","`nn.Linear(784, 128)`\n","\n","one layer linear model을 선언합니다.\n","\n","이 경우, input feature는 784개, output feature는 128개가 됩니다.\n","\n","`nn.ReLU()`, `nn.Softmax(dim=-1)`\n","\n","activation function 입니다.\n","\n","`def forward(self, x):`\n","\n","nn.Module 을 상속받은 class라면 구현해야 하는 method입니다.\n","\n","`python`의 `__call__()` method를 사용하여 호출되는데요.\n","\n","`forward` method 내에서 이루어지는 계산은 `autograd에` 의해 automatic differentiation이 이루어져 gradient를 추적하게 됩니다."]},{"cell_type":"markdown","metadata":{"id":"GaY8jcGnZnQC"},"source":["위 예시에서는 각각의 linear layer를 hard coding하여 선언하였는데요.\n","\n","layer depth가 깊어지고, 각각의 layer의 hyperparamter가 서로 다르면 손이 아플 수 있습니다.\n","\n","예를 들어 linear layer를 10개 이상 쌓은 model을 이런 식으로 작성하고 싶지는 않을 것입니다.\n","\n","이런 경우, model initialization을 편하게 할 수 있는 방법들을 사용하고 싶을 수 있습니다.\n","\n","아래 예시는 `nn.ModuleList` 를 활용해 linear layer initialization의 hard coding을 자동화한 것입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqC-NGiXZnQC"},"outputs":[],"source":["class MLP2(nn.Module):\n","    def __init__(self, hidden_features: list):\n","        super(MLP2, self).__init__()\n","        mlp = []\n","        for i in range(len(hidden_features) -1 ):\n","            mlp.append(nn.Linear(hidden_features[i], hidden_features[i + 1]))\n","            mlp.append(nn.ReLU())\n","        mlp.append(nn.Softmax(dim=-1))\n","\n","        self.mlp = nn.ModuleList(mlp)\n","\n","    def forward(self, x):\n","        for layer in self.mlp:\n","            x = layer(x)\n","        \n","        return x\n","    \n","mlp2 = MLP2([784] + [100 for i in range(8)] + [10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhlqvb1SZnQC","outputId":"4a0d7acd-e9fc-4a76-a221-cb353cd190f2"},"outputs":[{"data":{"text/plain":["MLP2(\n","  (mlp): ModuleList(\n","    (0): Linear(in_features=784, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=100, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=100, out_features=100, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=100, out_features=100, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=100, out_features=100, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=100, out_features=100, bias=True)\n","    (11): ReLU()\n","    (12): Linear(in_features=100, out_features=100, bias=True)\n","    (13): ReLU()\n","    (14): Linear(in_features=100, out_features=100, bias=True)\n","    (15): ReLU()\n","    (16): Linear(in_features=100, out_features=10, bias=True)\n","    (17): ReLU()\n","    (18): Softmax(dim=-1)\n","  )\n",")"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["mlp2"]},{"cell_type":"markdown","metadata":{"id":"waDh0unDA-5i"},"source":["linear layer의 feature수가 표기된 list[int]를 argument로 사용하면, \n","\n","아래 예시는 `nn.Sequential` 를 활용해 linear layer initialization의 hard coding을 자동화한 것입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FR88bgTNZnQC"},"outputs":[],"source":["class MLP3(nn.Module):\n","    def __init__(self, hidden_features: list):\n","        super(MLP3, self).__init__()\n","        mlp = []\n","        for i in range(len(hidden_features) -1 ):\n","            mlp.append(nn.Linear(hidden_features[i], hidden_features[i + 1]))\n","            mlp.append(nn.ReLU())\n","        mlp.append(nn.Softmax(dim=-1))\n","\n","        self.mlp = nn.Sequential(*mlp)\n","\n","    def forward(self, x):\n","        x = self.mlp(x)\n","        \n","        return x\n","    \n","mlp3 = MLP3([784] + [100 for i in range(8)] + [10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzO3e2QLZnQC","outputId":"256c1a15-c7d1-453c-ef87-5eb6649fbd90"},"outputs":[{"data":{"text/plain":["MLP3(\n","  (mlp): Sequential(\n","    (0): Linear(in_features=784, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=100, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=100, out_features=100, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=100, out_features=100, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=100, out_features=100, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=100, out_features=100, bias=True)\n","    (11): ReLU()\n","    (12): Linear(in_features=100, out_features=100, bias=True)\n","    (13): ReLU()\n","    (14): Linear(in_features=100, out_features=100, bias=True)\n","    (15): ReLU()\n","    (16): Linear(in_features=100, out_features=10, bias=True)\n","    (17): ReLU()\n","    (18): Softmax(dim=-1)\n","  )\n",")"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["mlp3"]},{"cell_type":"markdown","metadata":{"id":"-F3dvN5AZnQD"},"source":["# Convolutional Neural Network\n","\n","2D convolution과 max pooling operator로 구성된, CNN classifier을 정의하겠습니다.\n","\n","구조는 아래와 유사한데요.\n","\n","![image](https://drive.google.com/uc?export=download&id=1DKuuGYDMmV8q03EvWCamXAhSjRfgkIkB)\n","\n","2D convolution 을 정의할 때는 아래와 같은 정보가 필요합니다.\n","\n","- `Number of input channel`: 입력 데이터의 채널 개수입니다. 입력 데이터가 흑백 이미지인 경우, 채널은 하나입니다(grayscale). RGB로 표현된 컬러 이미지의 경우, 빨강(Red), 초록(Green), 파랑(Blue)의 각 색상 구성요소를 위해 3개의 채널이 있습니다.\n","- `Number of output channel`: 출력 데이터의 채널 개수입니다.\n","- `Size of kernel function`: 입력 데이터와 합성곱(convolution)을 수행하는 필터의 크기입니다. 3의 커널 크기는 한 번에 입력의 3x3 픽셀 영역을 조사하는 3x3 필터를 의미합니다.\n","\n","Max pooling을 정의할 때는 아래와 같은 정보가 필요합니다.\n","\n","- `Size of kernel function`: 위와 동일\n","- `Size of stride`: 커널 함수가 적용되는 픽셀 수입니다. 더 큰 스트라이드는 작은 스트라이드보다 특성 크기를 더 빠르게 감소시킵니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXfTysA0ZnQD"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        # input channel 1, output channel 6, kernel size 5\n","        self.conv1 = nn.Conv2d(1, 6, 5) \n","        self.pool = nn.MaxPool2d(2, 2) \n","        # input channel 6, output channel 16, kernel size 5\n","        self.conv2 = nn.Conv2d(6, 16, 5) \n","        self.fc1 = nn.Linear(16 * 4 * 4, 120) # an image size is reduced to 4x4 at this stage\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 4 * 4) # reshaping the tensor for the fully connected layer\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x) # no need to apply softmax, as it's included in the CrossEntropyLoss\n","        return x\n","\n","cnn = CNN()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uU8Go6uYZnQD","outputId":"ccf3c2cf-53bc-4190-fce4-769eb93926b1"},"outputs":[{"data":{"text/plain":["CNN(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=256, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")"]},"execution_count":174,"metadata":{},"output_type":"execute_result"}],"source":["cnn"]},{"cell_type":"markdown","metadata":{"id":"sbN1ymuJZnQO"},"source":["# Recurrent Neural Network\n","\n","RNN model의 구조는 대략 아래 그림과 같습니다.\n","\n","![image](https://drive.google.com/uc?export=download&id=18nFqcyTje4rv-DlFdFCBOMceYqXTjjqm)\n","\n","하나의 shared RNN Cell을 사용해, 이전 time step의 hidden state와 현재 time step의 input을 받으면 현재 time step의 output state가 계산되는데요.\n","\n","RNN model을 정의하는 데에는 아래와 같은 정보가 필요합니다.\n","\n","- `input_size`: Dimensionality of input feature\n","- `hidden_size`: Dimensionality of hidden feature, which is used between the input layer and output layer\n","- `num_layers`: The number of RNN layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OA_P9BgUZnQO"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, num_tokens, input_size, hidden_size, num_layers, dropout=0.5):\n","        super(RNN, self).__init__()\n","        self.ntoken = num_tokens\n","        self.drop = nn.Dropout(dropout)\n","        self.embedding = nn.Embedding(num_tokens, input_size)\n","        self.rnn = nn.RNN(\n","            input_size, hidden_size, num_layers, dropout=dropout\n","            )\n","        self.head_layer = nn.Linear(hidden_size, num_tokens)\n","\n","        self.nhid = hidden_size\n","        self.nlayers = num_layers\n","\n","\n","    def forward(self, input, hidden):\n","        emb = self.drop(self.embedding(input))\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        decoded = self.head_layer(output)\n","        decoded = self.head_layer.view(-1, self.ntoken)\n","        return F.log_softmax(decoded, dim=1), hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzp3RadOZnQO"},"outputs":[],"source":["rnn = RNN(10, 768, 768, 12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bd6hCVrRZnQO","outputId":"08e8e609-c97a-4bc9-9cbe-a6ff55a37c61"},"outputs":[{"data":{"text/plain":["RNN(\n","  (drop): Dropout(p=0.5, inplace=False)\n","  (embedding): Embedding(10, 768)\n","  (rnn): RNN(768, 768, num_layers=12, dropout=0.5)\n","  (head_layer): Linear(in_features=768, out_features=10, bias=True)\n",")"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["rnn"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}